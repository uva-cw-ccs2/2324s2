{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f47b20",
   "metadata": {},
   "source": [
    "### Exercise Tutorial Week 4\n",
    "# Getting some hands-on experience with supervised machine learning\n",
    "\n",
    "## Question 1. \n",
    "In this tutorial you will work with supervised machine learning. We will classify tweets into four categories namely: normal, abusive, hateful, and spam.\n",
    "\n",
    "As you noted when reading the literature assigned for this week, there are a few steps that we need to take before we can use supervised machine learning. Namely:\n",
    "* Collect data (in CS often texts, e.g., tweets)\n",
    "* Develop a codebook and hand-code the data\n",
    "\n",
    "\n",
    "In this tutorial, we focus on the actual machine learning part of the process. Hence, we will use a database that already has been coded by humans. It contains tweets and each tweet has a label indicating to which of four categories it belongs, namely normal, abusive, hateful, or spam. Hence, we skip the first two steps of the process described above.\n",
    "\n",
    "Download the data for this exercise named \"hatespeech_text_label_vote_RESTRICTED_100K.csv\". \n",
    "These datafiles were retrieved from: https://www.dropbox.com/sh/4mapojr85a6sc76/AABYMkjLVG-HhueAgd0qM9kwa?dl=0![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Using the examples from this week's lecture, can you write a script that opens each file and:\n",
    "* Creates one list with the tweets\n",
    "* Creates one list with the labels of the tweets\n",
    "\n",
    "\n",
    "What could you do to check that this process went well? Can you explore the data a bit (i.e., by checking how often each label is present in the different datasets)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2112676",
   "metadata": {},
   "source": [
    "## Question 2.\n",
    "Now that we read in the data, we will proceed to the next step: Splitting our data into a training set and a test set. Luckily, scikit learn has a function that can do so for us! Run the code presented in the next blok to split up the dataset.\n",
    "        \n",
    "* What do these lines of code do?\n",
    "* Do you know what the random_state part refers to? Why is this useful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b45e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets_train, tweets_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff085196",
   "metadata": {},
   "source": [
    "## Question 3.\n",
    "On to the next step: Transforming the text into numbers, or setting up a vectorizer. Can you create some code that uses a count vectorizer on the texts that you read in? Hint: check out the example provided in the slides of this week! Doing so, you will see that the stopwords are defined (as a built-in stop word list). Why is that done? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30d61e",
   "metadata": {},
   "source": [
    "## Question 4.\n",
    "Now, let’s train a classifier and run it on the test data! Can you use the examples this week's lecture to train a Naïve Bayes classifer with our count vectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177430c6",
   "metadata": {},
   "source": [
    "## Question 5.\n",
    "When you run the code you created for the previous question, you will see that it prints no output. How do you know if your code worked? Run the code presented in the next block (depending on how you named your labels, you may need to adjust the arguments).\n",
    "\n",
    "\n",
    "Check out the documentation of the scikit learn package: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "What is do the numbers in the output mean? What can you do with it?\n",
    "We will discuss this (and more) in next week's lecture! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86eb3389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal' 'normal' 'normal' 'normal' 'spam' 'normal' 'normal' 'normal'\n",
      " 'abusive' 'normal']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.81      0.88      0.85      5369\n",
      "     hateful       0.83      0.05      0.10       966\n",
      "      normal       0.78      0.93      0.85     10848\n",
      "        spam       0.67      0.30      0.41      2817\n",
      "\n",
      "    accuracy                           0.78     20000\n",
      "   macro avg       0.77      0.54      0.55     20000\n",
      "weighted avg       0.78      0.78      0.75     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(y_pred[:10])\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf587f3",
   "metadata": {},
   "source": [
    "### About this exercise:\n",
    "This exercise is based on the materials developed and the texts written by Wouter van Atteveldt, Damian Trilling and Carlos Arcila Calderon as reported in their book 'Computational Analysis of Communication' (2022, Wiley-Blackwell)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
