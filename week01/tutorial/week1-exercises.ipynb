{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756eddab",
   "metadata": {},
   "source": [
    "## Exercises week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca24858",
   "metadata": {},
   "source": [
    "# Working with textual data\n",
    "\n",
    "### 0. Get the data.\n",
    "\n",
    "- Download  `articles.tar.gz` or `articles.zip` from Canvas (under `Week 1`). Please note that this is not the full dataset, but random sample of the data described [here](https://dx.doi.org/10.7910/DVN/ULHLCB).\n",
    "\n",
    "\n",
    "<div class=\"alert-danger\">\n",
    "<p>Alternatively, you can also download <code>articles.tar.gz</code> from\n",
    "<a href=\"https://dx.doi.org/10.7910/DVN/ULHLCB\">https://dx.doi.org/10.7910/DVN/ULHLCB</a> to get <strong>all</strong> the data. Please note that this is a very large dataset, and for practice purposes, you do not need everything. </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "- Unpack it. On Linux and MacOS, you can do this with `tar -xzf mydata.tar.gz` on the command line. On Windows, you may need an additional tool such as `7zip` for that (note that technically speaking, there is a `tar` archive within a `gz` archive, so unpacking may take *two* steps depending on your tool).\n",
    "\n",
    "\n",
    "### 1. Inspect the structure of the dataset.\n",
    "What information do the following elements give you?\n",
    "\n",
    "- folder (directory) names\n",
    "- folder structure/hierarchy\n",
    "- file names\n",
    "- file contents\n",
    "\n",
    "### 2. Discuss strategies for working with this dataset!\n",
    "\n",
    "- Which questions could you answer?\n",
    "- How could you deal with it, given the size and the structure?\n",
    "\n",
    "### 3. Read some (or all?) data\n",
    "\n",
    "Here is some example code that you can modify. you could, for instance, do the following to read a *part* of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "#specify the path to your unpacked articles.\n",
    "PATH = 'articles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85824906",
   "metadata": {},
   "outputs": [],
   "source": [
    "newspaperfiles = glob(PATH+'/*/Vox/*')\n",
    "documents = []\n",
    "for filename in newspaperfiles:\n",
    "    with open(filename) as f:\n",
    "        documents.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f82c83a2",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "<ul>\n",
    "<li>Can you explain what the <code>glob</code> function does?</li>\n",
    "<li>Can you modify the code so to read in e.g., <code>The Guardian</code> or another news source in the folder <code>articles</code>?</li>\n",
    "<li>What does <code>documents</code> contain? First make an educated guess based on the code snippet, then check it! Do <em>not</em> print the whole thing, but use <code>len()</code>, <code>type()</code> en slicing (e.g.,<code>[:10]</code>) to get the info you need.</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edb7cf",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert-block alert-warning\">\n",
    "<p>Tip: take a random sample of the articles for practice purposes (if your code works, you can scale up!)</p><code>articles =random.sample(documents, 10)</code></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59497fad",
   "metadata": {},
   "source": [
    "### 4. Perform some analyses!\n",
    "\n",
    "- Perform some first analyses on the data using string methods and regular expressions!\n",
    "\n",
    "Techniques you can try out include:\n",
    "\n",
    "a.  lowercasing\n",
    "\n",
    "b.  tokenization\n",
    "\n",
    "c.  stopword removal\n",
    "\n",
    "d.  stemming and/or lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles =random.sample(documents, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3789180",
   "metadata": {},
   "source": [
    "    #a. lowercasing articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_lower_cased = []\n",
    "for art in articles:\n",
    "    articles_lower_cased.append(art.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d98203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same, using list comprehension:\n",
    "articles_lower_cased = [art.lower() for art in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da6d95",
   "metadata": {},
   "source": [
    "    #b. tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Basic approach to tokenization\n",
    "articles_split = []\n",
    "for art in articles:\n",
    "    articles_split.append(art.split())\n",
    "\n",
    "# make sure to print often to check your progress:\n",
    "# print(articles_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, using list comprehension:\n",
    "articles_split = [art.split() for art in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb9c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#. More advanced approach to tokenization\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "articles_tokenized = []\n",
    "for art in articles:\n",
    "    articles_tokenized.append(tokenizer.tokenize(art))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb44f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Same, but using list comprehension:\n",
    "articles_tokenized = [tokenizer.tokenize(art) for art in articles ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0371f8ef",
   "metadata": {},
   "source": [
    "    #c. stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78867ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the a stopword list\n",
    "mystopwords = stopwords.words(\"english\")\n",
    "\n",
    "#check what is in there:\n",
    "print(mystopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually add more stopwords to your list if needed:\n",
    "mystopwords.extend([\"add\", \"more\", \"words\"]) \n",
    "print(mystopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, remove these stopwords from the corpus:\n",
    "\n",
    "articles_without_stopwords = []\n",
    "for article in articles:\n",
    "    articles_no_stop = \"\"\n",
    "    for word in article.lower().split():\n",
    "        if word not in mystopwords:\n",
    "            articles_no_stop = articles_no_stop + \" \" + word\n",
    "    articles_without_stopwords.append(articles_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8618beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as the cell above, this time with list comprehension\n",
    "\n",
    "articles_without_stopwords = [\" \".join([w for w in article.lower().split() if w not in mystopwords]) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f455dd",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<div class=\"alert-block alert-warning\">\n",
    "It's good practice to frequently inspect the results of your code, to make sure you are not making mistakes, and the results make sense. For example, compare your results to some random articles from the original sample:\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles[8][:100])\n",
    "print(\"-----------------\")\n",
    "print(\"\".join(articles_without_stopwords[8])[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9a2702f",
   "metadata": {},
   "source": [
    "    #d. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8304641",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stemmed_text = []\n",
    "for article in articles:\n",
    "    stemmed_words = \"\"\n",
    "    for word in article.lower().split():\n",
    "        stemmed_words = stemmed_words + \" \" + stemmer.stem(word)\n",
    "    stemmed_text.append(stemmed_words.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28de46",
   "metadata": {},
   "source": [
    "same as the cell above, this time with list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ab0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text  = [\" \".join([stemmer.stem(w) for w in article.lower().split()]) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682deb4",
   "metadata": {},
   "source": [
    "Please note that alternative stemmers are available through the `nltk` library. \n",
    "E.g., you can try experimenting with `NLTK's Porter Stemmer`:\n",
    "\n",
    "```python\n",
    "porter_stemmer = nltk.stem.PorterStemmer()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d8846",
   "metadata": {},
   "source": [
    "## OPTIONAL:\n",
    "\n",
    "If you want to try out Lemmatization, you need to download a language model from Spacy. \n",
    "\n",
    "Please run the following command in your terminal:\n",
    "\n",
    "`python3 -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Initialize spaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a sentence to be stemmed and lemmatized. In this case, we will try with a single random article: articles[1]\n",
    "# Use spaCy's English language model to lemmatize the sentence\n",
    "lemmatized_sentence = \" \".join([token.lemma_ for token in nlp(articles[1])])\n",
    "\n",
    "# Print the original sentence, stemmed sentence, and lemmatized sentence\n",
    "print(articles[1][0:100])\n",
    "#print(\"**********\")\n",
    "print(lemmatized_sentence[0:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
