{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d97751",
   "metadata": {},
   "source": [
    "### Exercise Tutorial Week 6\n",
    "# Getting some hands-on experience with supervised machine learning\n",
    "\n",
    "## Question 1. \n",
    "In last week's exercise, you set up a classifier using the hatespeech dataset (retrieved from: https://www.dropbox.com/sh/4mapojr85a6sc76/AABYMkjLVG-HhueAgd0qM9kwa?dl=0![image-2.png](attachment:image-2.png))\n",
    "The classifier was based on a count vectorizer using Naïve Bayes. For this exercise, you created the following code:\n",
    "\n",
    "```python\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "file = \"hatespeech_text_label_vote_RESTRICTED_100K.csv\"\n",
    "tweets = []\n",
    "labels = []\n",
    "\n",
    "with open(file) as fi:\n",
    "    data = csv.reader(fi, delimiter='\\t')\n",
    "    for row in data:\n",
    "        tweets.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "print(len(tweets) == len(labels)) # there should be just as many tweets as there are labels\n",
    "\n",
    "Counter(labels)\n",
    "plt.bar(Counter(labels).keys(), Counter(labels).values())\n",
    "\n",
    "\n",
    "# splitting up the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets_train, tweets_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Classifier with a count vectorizer and Naïve Bayes\n",
    "from sklearn.feature_extraction.text import (CountVectorizer)\n",
    "\n",
    "countvectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X_train = countvectorizer.fit_transform(tweets_train)\n",
    "X_test = countvectorizer.transform(tweets_test)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "\n",
    "# Print a classification report for the classifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Can you use the examples from last week's slides and train another classifier based on Logistic Regression and a tf-idf vectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaaf5de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.87      0.89      0.88      5369\n",
      "     hateful       0.67      0.25      0.36       966\n",
      "      normal       0.82      0.89      0.85     10848\n",
      "        spam       0.57      0.48      0.52      2817\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.73      0.63      0.66     20000\n",
      "weighted avg       0.79      0.80      0.79     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Model answer.\n",
    "\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer)\n",
    "from sklearn.linear_model import (LogisticRegression)\n",
    "\n",
    "\n",
    "Tfidfvectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X_train = Tfidfvectorizer.fit_transform(tweets_train)\n",
    "X_test = Tfidfvectorizer.transform(tweets_test)\n",
    "\n",
    "logres = LogisticRegression(max_iter=1000)\n",
    "logres.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logres.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fe6a2",
   "metadata": {},
   "source": [
    "## Question 2.\n",
    "\n",
    "As you saw in the article by Meppelink et al. (2021), we can try different combinations of these models (Naïve Bayes and Logistic Regression) and vectorizers (count and tf-idf). If you want to use Naïve Bayes and Logistic Regression as the models for a classifier, and a count vectorizer and a tf-idf vectorizer, how many classifiers could you then train? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8d5c5",
   "metadata": {},
   "source": [
    "### Model answer.\n",
    "\n",
    "You could then train 2 (count vectorzier vs. tf-idf vectorizer) * 2 (Naïve Bayes vs. Logistic Regression) = 4 classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e29260",
   "metadata": {},
   "source": [
    "## Question 3.\n",
    "\n",
    "We could simply copy-paste the code used in the previous questions and adjust it for each of the classifiers. However, a cleaner approach is to write a function in which we define the specifics of each classifier. The code below does that.\n",
    "\n",
    "In this code, we create a loop that trains each classifier by calling the function that is build in the first part of the code. \n",
    "\n",
    "Run the code below and compare it to the code that you wrote to train one classifier: Do you understand what is happening there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09348f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.80      0.88      0.84      5369\n",
      "     hateful       0.41      0.28      0.33       966\n",
      "      normal       0.85      0.79      0.82     10848\n",
      "        spam       0.53      0.63      0.57      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.65      0.64      0.64     20000\n",
      "weighted avg       0.77      0.77      0.77     20000\n",
      "\n",
      "\n",
      "\n",
      "NB-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.81      0.81      0.81      5369\n",
      "     hateful       0.87      0.05      0.09       966\n",
      "      normal       0.76      0.92      0.83     10848\n",
      "        spam       0.65      0.32      0.43      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.77      0.53      0.54     20000\n",
      "weighted avg       0.76      0.77      0.73     20000\n",
      "\n",
      "\n",
      "\n",
      "LR-Count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.87      0.91      0.89      5369\n",
      "     hateful       0.63      0.28      0.38       966\n",
      "      normal       0.81      0.91      0.86     10848\n",
      "        spam       0.58      0.39      0.47      2817\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.72      0.62      0.65     20000\n",
      "weighted avg       0.79      0.80      0.79     20000\n",
      "\n",
      "\n",
      "\n",
      "LR-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.88      0.89      0.89      5369\n",
      "     hateful       0.73      0.20      0.32       966\n",
      "      normal       0.80      0.94      0.86     10848\n",
      "        spam       0.66      0.35      0.46      2817\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.77      0.60      0.63     20000\n",
      "weighted avg       0.80      0.81      0.78     20000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import (CountVectorizer)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer)\n",
    "from sklearn.linear_model import (LogisticRegression)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "configs = [\n",
    "  (\"NB-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"NB-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"LR-Count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\")),\n",
    "  (\"LR-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\"))]\n",
    "\n",
    "\n",
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    X_train = vectorizer.fit_transform(tweets_train)\n",
    "    X_test = vectorizer.transform(tweets_test)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c8380",
   "metadata": {},
   "source": [
    "## Question 4.\n",
    "\n",
    "Check out the documentation of scikit learn (https://scikit-learn.org/stable/supervised_learning.html). Can you try to use other models and train a classifier with them? Can you merge this code into the code used in the previous question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a04046d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.80      0.88      0.84      5369\n",
      "     hateful       0.41      0.28      0.33       966\n",
      "      normal       0.85      0.79      0.82     10848\n",
      "        spam       0.53      0.63      0.57      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.65      0.64      0.64     20000\n",
      "weighted avg       0.77      0.77      0.77     20000\n",
      "\n",
      "\n",
      "\n",
      "NB-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.81      0.81      0.81      5369\n",
      "     hateful       0.87      0.05      0.09       966\n",
      "      normal       0.76      0.92      0.83     10848\n",
      "        spam       0.65      0.32      0.43      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.77      0.53      0.54     20000\n",
      "weighted avg       0.76      0.77      0.73     20000\n",
      "\n",
      "\n",
      "\n",
      "LR-count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.87      0.91      0.89      5369\n",
      "     hateful       0.63      0.28      0.38       966\n",
      "      normal       0.81      0.91      0.86     10848\n",
      "        spam       0.58      0.39      0.47      2817\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.72      0.62      0.65     20000\n",
      "weighted avg       0.79      0.80      0.79     20000\n",
      "\n",
      "\n",
      "\n",
      "LR-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.88      0.89      0.89      5369\n",
      "     hateful       0.73      0.20      0.32       966\n",
      "      normal       0.80      0.94      0.86     10848\n",
      "        spam       0.66      0.35      0.46      2817\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.77      0.60      0.63     20000\n",
      "weighted avg       0.80      0.81      0.78     20000\n",
      "\n",
      "\n",
      "\n",
      "KNN-count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.60      0.85      0.70      5369\n",
      "     hateful       0.44      0.14      0.22       966\n",
      "      normal       0.73      0.64      0.68     10848\n",
      "        spam       0.28      0.27      0.28      2817\n",
      "\n",
      "    accuracy                           0.62     20000\n",
      "   macro avg       0.52      0.48      0.47     20000\n",
      "weighted avg       0.62      0.62      0.61     20000\n",
      "\n",
      "\n",
      "\n",
      "KNN-tfidf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.81      0.47      0.59      5369\n",
      "     hateful       0.54      0.14      0.22       966\n",
      "      normal       0.65      0.94      0.77     10848\n",
      "        spam       0.52      0.17      0.26      2817\n",
      "\n",
      "    accuracy                           0.67     20000\n",
      "   macro avg       0.63      0.43      0.46     20000\n",
      "weighted avg       0.67      0.67      0.62     20000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Model answer.\n",
    "\n",
    "# Various answers are possible here, but let's try a model based on K nearest neighbours for example:\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "configs = [\n",
    "  (\"NB-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"NB-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"LR-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\")),\n",
    "  (\"LR-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\")),\n",
    "  (\"KNN-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "  KNeighborsClassifier(n_neighbors=3)),\n",
    "  (\"KNN-tfidf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "  KNeighborsClassifier(n_neighbors=3))]\n",
    "\n",
    "\n",
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    X_train = vectorizer.fit_transform(tweets_train)\n",
    "    X_test = vectorizer.transform(tweets_test)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\") \n",
    "\n",
    "\n",
    "# Note that only the first part of the code needs to be adjusted!\n",
    "# Also note that running the KNN classifier takes a bit of time...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf209f7e",
   "metadata": {},
   "source": [
    "## Question 5.\n",
    "\n",
    "Based on the output that the classifier prints, what classifier performs the best? In your answer, consider:\n",
    "* What information you need to identify the best classifier\n",
    "* What metric you base your conclusion (i.e., precision, recall, accurcay, or F1-score) on and why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30a907-5d0a-43fd-8cc1-50ca65bb0fbb",
   "metadata": {},
   "source": [
    "### Model answer.\n",
    "\n",
    "Depending on the metric that you base your evaluation on, a different classifier could be considered best. \n",
    "The best metric in turn, differs depending on what a classifier is used for. \n",
    "\n",
    "In this case, no specific information about the usage of the classifier is provided. \n",
    "Therefore, the F1-score may be the best metric to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977fce73",
   "metadata": {},
   "source": [
    "## Question 6.\n",
    "\n",
    "Let's say that you base your evaluation on the F1-score of the classifier. You can choose between the macro average and the weighted average of the F1-score. Check out the scikit learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support). What F1-value (macro average or weighted average) would you select?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ddc2e-d233-4817-a0b9-b4d886348593",
   "metadata": {},
   "source": [
    "### Model answer.\n",
    "\n",
    "From the documentation:\n",
    "    \n",
    "'macro':\n",
    "Calculate metrics for each label, and find their unweighted mean. \n",
    "This does not take label imbalance into account.\n",
    "\n",
    "'weighted':\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). \n",
    "This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "                        \n",
    "                        \n",
    "It seems that there are relaitvely many normal tweets present in the dataset. To account for this, the weighted average may be a good choide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92830270",
   "metadata": {},
   "source": [
    "## Question 7.\n",
    "\n",
    "When looking at the classification report, you will see another column indicating values for something labelled 'support'.\n",
    "Can you do some searching online and find out what 'support' is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a349ab-b357-4230-8fbe-90e5e5f1b78f",
   "metadata": {},
   "source": [
    "### Model answer.\n",
    "\n",
    "Support refers to the number of true instances for each label (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe8fa3",
   "metadata": {},
   "source": [
    "## Question 8.\n",
    "\n",
    "Two researchers want to use the classifier to distinguish between tweets that are spam or hateful and tweets that are not (either because they are normal or abusive). \n",
    "They are, however, not happy with the performance of the classifier when looking at the accuracy, precision, and recall for the spam category or the hateful category.\n",
    "One of the researchers suggests to first recode the labels, so that all tweets that were annotated as spam receive a label 'spam' or 'hateful' are grouped together and all other tweets are grouped together as well.\n",
    "\n",
    "What would the consequences be of doing so?\n",
    "What can you do to check your own answer? Try to recode the labels and see what happens!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50824f56-52a5-4ca4-aa44-0e8866c11a84",
   "metadata": {},
   "source": [
    "### Model answer.\n",
    "\n",
    "It could increase the performance of the machine because the groups are no made larger and more easy to dinstinguish...\n",
    "Or the groups are made too larger and therefore too vague, making it harder to distinguish between categories, decreasing the\n",
    "performance of the machine...\n",
    "\n",
    "You can recode the labels and train/validate the machines again to see what happens! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80781bc5",
   "metadata": {},
   "source": [
    "## Question 9.\n",
    "For now, let's say that the classifier based on a count vectorizer and Logistic Regression is the one we prefer. We now want to use this model to predict the label for new data that we have not annotated (remember, this was the whole goal of SML)!\n",
    "\n",
    "To do this, let’s save our classifier and our vectorizer to a file. If we don’t do this, we would need to re-train our model every time we want to use it. This is not so convenient, for example, we would always need to have our training data at hand. The code below shows you how to make a vectorizer and train a classifier (a repetition of what we did before to show you the whole process) and store them into a file.\n",
    "\n",
    "In the code, you will see that both the classifier and the vectorizer are stored into a file. Why do you need to store both (why not just store the classifier only)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858d4a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This Tweet is very shitty nasty mean and hateful' is probably 'abusive'.\n",
      "'This is a very normal normal tweet.' is probably 'normal'.\n",
      "'2%^&GHJ &(&hrqjf3 click this link' is probably 'spam'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Make a vectorizer and train a classifier\n",
    "vectorizer=CountVectorizer(min_df=5, max_df=.5)\n",
    "classifier=LogisticRegression(solver=\"liblinear\")\n",
    "X_train=vectorizer.fit_transform(tweets_train)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save them to disk\n",
    "with open(\"myvectorizer.pkl\",mode=\"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open(\"myclassifier.pkl\",mode=\"wb\") as f:\n",
    "    joblib.dump(classifier, f)\n",
    "\n",
    "# Later on, re-load this classifier and apply:\n",
    "new_tweets = [\"This Tweet is very shitty nasty mean and hateful\", \n",
    "            \"This is a very normal normal tweet.\", \n",
    "            \"2%^&GHJ &(&hrqjf3 click this link\"]\n",
    "\n",
    "with open(\"myvectorizer.pkl\",mode=\"rb\") as f:\n",
    "    myvectorizer = pickle.load(f)\n",
    "with open(\"myclassifier.pkl\",mode=\"rb\") as f:\n",
    "    myclassifier = joblib.load(f)\n",
    "    \n",
    "new_features = myvectorizer.transform(new_tweets)\n",
    "pred = myclassifier.predict(new_features)\n",
    "\n",
    "for tweet, label in zip(new_tweets, pred):\n",
    "    print(f\"'{tweet}' is probably '{label}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af480c",
   "metadata": {},
   "source": [
    "### Model answer.\n",
    "\n",
    "The vectorizer needs to be stored as well. Remember what we talked about earlier: once a vectorizer is fit on certain data, fitting it again on new data changes the set up of the vectorizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d01dc",
   "metadata": {},
   "source": [
    "### About this exercise:\n",
    "\n",
    "This exercise is based on the materials developed and the texts written by Wouter van Atteveldt, Damian Trilling and Carlos Arcila Calderon as reported in their book 'Computational Analysis of Communication' (2022, Wiley-Blackwell)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
